{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pediatric-couple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "PyTorch Version: 1.8.1\n",
      "CuDNN Version: 8005\n",
      "gpu usage (current/max): 1.12 / 2.39 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from typing import Callable, Any, Optional, List\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "direct-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Atrous Spatial Pyramid Pooling (Segmentation Network)\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        modules = [\n",
    "            nn.Conv3d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()\n",
    "        ]\n",
    "        super(ASPPConv, self).__init__(*modules)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPPPooling, self).__init__(\n",
    "            #nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-3:]\n",
    "        x = F.adaptive_avg_pool3d(x,(1))\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='nearest')#, align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, atrous_rates, out_channels=256):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = []\n",
    "        modules.append(nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU()))\n",
    "\n",
    "        rates = tuple(atrous_rates)\n",
    "        for rate in rates:\n",
    "            modules.append(ASPPConv(in_channels, out_channels, rate))\n",
    "\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv3d(len(self.convs) * out_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "introductory-entrepreneur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#CNN layer 24\n"
     ]
    }
   ],
   "source": [
    "#Mobile-Net with depth-separable convolutions and residual connections\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.module(inputs) + inputs\n",
    "    \n",
    "in_channels = torch.Tensor([1,16,24,24,32,32,32,64]).long()\n",
    "mid_channels = torch.Tensor([32,96,144,144,192,192,192,384]).long()\n",
    "out_channels = torch.Tensor([16,24,24,32,32,32,64,64]).long()\n",
    "mid_stride = torch.Tensor([1,1,1,1,1,2,1,1])\n",
    "\n",
    "net = []\n",
    "net.append(nn.Identity())\n",
    "for i in range(len(in_channels)):\n",
    "    inc = int(in_channels[i]); midc = int(mid_channels[i]); outc = int(out_channels[i]); strd = int(mid_stride[i])\n",
    "    layer = nn.Sequential(nn.Conv3d(inc,midc,1,bias=False),nn.BatchNorm3d(midc),nn.ReLU6(True),\\\n",
    "                    nn.Conv3d(midc,midc,3,stride=strd,padding=1,bias=False,groups=midc),nn.BatchNorm3d(midc),nn.ReLU6(True),\\\n",
    "                                   nn.Conv3d(midc,outc,1,bias=False),nn.BatchNorm3d(outc))\n",
    "    if(i==0):\n",
    "        layer[0] = nn.Conv3d(inc,midc,3,padding=1,stride=2,bias=False)\n",
    "    if((inc==outc)&(strd==1)):\n",
    "        net.append(ResBlock(layer))\n",
    "    else:\n",
    "        net.append(layer)\n",
    "\n",
    "backbone = nn.Sequential(*net)\n",
    "\n",
    "count = 0\n",
    "# weight initialization\n",
    "for m in backbone.modules():\n",
    "    if isinstance(m, nn.Conv3d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "        count += 1\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.BatchNorm3d, nn.GroupNorm)):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "print('#CNN layer',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "removable-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete model: MobileNet + ASPP + head (with a single skip connection)\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "#newer model (one more stride, no groups in head)\n",
    "aspp = ASPP(64,(2,4,8,16),128)#ASPP(64,(1,),128)#\n",
    "num_classes = 26#14 # 25 for verse19 and 29 for verse20\n",
    "head = nn.Sequential(nn.Conv3d(128+16, 64, 1, padding=0,groups=1, bias=False),nn.BatchNorm3d(64),nn.ReLU(),\\\n",
    "                     nn.Conv3d(64, 64, 3, groups=1,padding=1, bias=False),nn.BatchNorm3d(64),nn.ReLU(),\\\n",
    "                     nn.Conv3d(64, num_classes, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "flying-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Identity()\n",
      "  (1): Sequential(\n",
      "    (0): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "    (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "    (4): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU6(inplace=True)\n",
      "    (6): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (7): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv3d(16, 96, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "    (3): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)\n",
      "    (4): BatchNorm3d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU6(inplace=True)\n",
      "    (6): Conv3d(96, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (7): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (3): ResBlock(\n",
      "    (module): Sequential(\n",
      "      (0): Conv3d(24, 144, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "      (3): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=144, bias=False)\n",
      "      (4): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU6(inplace=True)\n",
      "      (6): Conv3d(144, 24, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (7): BatchNorm3d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (4): Sequential(\n",
      "    (0): Conv3d(24, 144, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "    (3): Conv3d(144, 144, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=144, bias=False)\n",
      "    (4): BatchNorm3d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU6(inplace=True)\n",
      "    (6): Conv3d(144, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (7): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (5): ResBlock(\n",
      "    (module): Sequential(\n",
      "      (0): Conv3d(32, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "      (3): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=192, bias=False)\n",
      "      (4): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU6(inplace=True)\n",
      "      (6): Conv3d(192, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (7): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): Sequential(\n",
      "    (0): Conv3d(32, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "    (3): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=192, bias=False)\n",
      "    (4): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU6(inplace=True)\n",
      "    (6): Conv3d(192, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (7): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (7): Sequential(\n",
      "    (0): Conv3d(32, 192, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "    (3): Conv3d(192, 192, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=192, bias=False)\n",
      "    (4): BatchNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU6(inplace=True)\n",
      "    (6): Conv3d(192, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (8): ResBlock(\n",
      "    (module): Sequential(\n",
      "      (0): Conv3d(64, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "      (3): Conv3d(384, 384, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=384, bias=False)\n",
      "      (4): BatchNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU6(inplace=True)\n",
      "      (6): Conv3d(384, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (7): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ") ASPP(\n",
      "  (convs): ModuleList(\n",
      "    (0): Sequential(\n",
      "      (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): ASPPConv(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(2, 2, 2), dilation=(2, 2, 2), bias=False)\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): ASPPConv(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(4, 4, 4), dilation=(4, 4, 4), bias=False)\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): ASPPConv(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(8, 8, 8), dilation=(8, 8, 8), bias=False)\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): ASPPConv(\n",
      "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(16, 16, 16), dilation=(16, 16, 16), bias=False)\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (5): ASPPPooling(\n",
      "      (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (project): Sequential(\n",
      "    (0): Conv3d(768, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "    (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ") Sequential(\n",
      "  (0): Conv3d(144, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "  (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "  (4): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Conv3d(64, 26, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(backbone, aspp, head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automated-evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001216 155584 121754\n"
     ]
    }
   ],
   "source": [
    "def countParameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "print(countParameters(aspp),countParameters(backbone),countParameters(head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "musical-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerSe_Dataset(Dataset):\n",
    "    \"\"\"VerSe Dataset already preprocessed to image spacing 1.5x1.5x1.5\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.img_path = []\n",
    "        self.counter = 0\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for filename in files:\n",
    "                    path =  root + '/' + filename\n",
    "                    if \"msk\" not in path and 'nii.gz' in path:\n",
    "                        self.counter += 1\n",
    "                        self.img_path.append(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.counter\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_path[idx]\n",
    "        seg_name = self.img_path[idx].split('.nii.gz')[0] + '_msk.nii.gz'\n",
    "    \n",
    "        image = nib.load(img_name).get_fdata()[:, :, :]\n",
    "        label = nib.load(seg_name).get_fdata()[:, :, :]\n",
    "        \n",
    "        #normalise the image \n",
    "        image = (2*(image-image.min())/ (image.max() - image.min()) - 1)\n",
    "    \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        image = torch.from_numpy(image).unsqueeze(0)\n",
    "        label = torch.from_numpy(label).unsqueeze(0)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class AffineAugmentation(object):\n",
    "    def __init__(self, factor=None):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        label = sample['label']\n",
    "\n",
    "        D,H,W = image.shape\n",
    "        B = 1\n",
    "        C = 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            affine = F.affine_grid(torch.eye(3, 4).unsqueeze(0) + torch.randn(B, 3, 4) * .07, (B, C, D, H, W)).cuda()\n",
    "            img = F.grid_sample(torch.from_numpy(image).unsqueeze(0).unsqueeze(0).cuda().float(), affine, padding_mode='zeros', mode='bilinear', align_corners = False).squeeze(0)\n",
    "            lab = F.grid_sample(torch.from_numpy(label).unsqueeze(0).unsqueeze(0).cuda().float(), affine, padding_mode='zeros', mode='nearest', align_corners = False).squeeze(0)\n",
    "            #back to numpy for cropping\n",
    "            img = img.squeeze(0).squeeze(0).cpu().detach().numpy()\n",
    "            lab = lab.squeeze(0).squeeze(0).cpu().detach().numpy()\n",
    "            new_sample = {'image': img, 'label': lab}\n",
    "        return new_sample\n",
    "    \n",
    "class RandomCrop(object):\n",
    "    \"\"\"Random crop of sample image and label.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 3\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        d, h, w = image.shape\n",
    "        new_d, new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        depth = np.random.randint(0, d - new_d)\n",
    "\n",
    "        image = image[depth: depth + new_d, top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        label = label[depth: depth + new_d, top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "    \n",
    "class ZeroPad(object):\n",
    "    \"\"\"ZeroPad a sample. Pad smaller images to fit to the patch size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 3\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        d, h, w = image.shape\n",
    "        \n",
    "        max_d = np.maximum(d,self.output_size[0])\n",
    "        max_h = np.maximum(h,self.output_size[0])\n",
    "        max_w = np.maximum(w,self.output_size[0])\n",
    "        \n",
    "        new_img = np.zeros((max_d,max_h,max_w))\n",
    "        new_lab = np.zeros((max_d,max_h,max_w))\n",
    "        \n",
    "        d2= h2= w2 = 0\n",
    "        \n",
    "        if d < self.output_size[0]:\n",
    "            d2 = int((self.output_size[0] - d)/2)\n",
    "        if h < self.output_size[1]:\n",
    "            h2 = int((self.output_size[1] - h)/2)\n",
    "        if w < self.output_size[2]:\n",
    "            w2 = int((self.output_size[2] - w)/2)\n",
    "            \n",
    "\n",
    "        new_img[d2:d2+d, h2:h2+h, w2:w2+w] = image\n",
    "        new_lab[d2:d2+d, h2:h2+h, w2:w2+w] = label\n",
    "\n",
    "\n",
    "        return {'image': new_img, 'label': new_lab}\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        return {'image': torch.from_numpy(image).unsqueeze(0),\n",
    "                'label': torch.from_numpy(label).unsqueeze(0)}\n",
    "\n",
    "verse19_dataset = VerSe_Dataset(root_dir='Path To Preprocessed VerSe Data')# without transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interstate-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dataset = VerSe_Dataset(root_dir= 'Path To Preprocessed VerSe Data',\n",
    "                                           transform=transforms.Compose([\n",
    "                                               AffineAugmentation(),\n",
    "                                               ZeroPad(132),\n",
    "                                               RandomCrop(128),\n",
    "                                               ToTensor()\n",
    "                                           ]))\n",
    "dataloader = DataLoader(transformed_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pharmaceutical-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaySegment(gray1,seg1,colors,flag=False):\n",
    "    H, W = seg1.squeeze().size()\n",
    "    #colors=torch.FloatTensor([0,0,0,199,67,66,225,140,154,78,129,170,45,170,170,240,110,38,111,163,91,235,175,86,202,255,52,162,0,183]).view(-1,3)/255.0\n",
    "    segs1 = F.one_hot(seg1.long(),29).float().permute(2,0,1)\n",
    "\n",
    "    seg_color = torch.mm(segs1.view(29,-1).t(),colors).view(H,W,3)\n",
    "    alpha = torch.clamp(1.0 - 0.5*(seg1>0).float(),0,1.0)\n",
    "\n",
    "    overlay = (gray1*alpha).unsqueeze(2) + seg_color*(1.0-alpha).unsqueeze(2)\n",
    "    if(flag):\n",
    "        plt.imshow((overlay).numpy()); \n",
    "        plt.axis('off');\n",
    "        plt.show()\n",
    "    return overlay\n",
    "\n",
    "label_rgb = torch.Tensor([0,0,0, 208,0,0, 255,230,2, 48,194,0, 0,110,255, 98,0,190, 247,0,255, 158,97,0, 255,160,70, \\\n",
    "                         0,131,32, 49,165,171,  62,27,203, 107,0,208, 0,110,255, 98,0,190, 247,0,255, 158,97,0, 255,160,70, \\\n",
    "                         0,131,32, 49,165,171,  62,27,203, 107,0,208, 208,0,0, 255,230,2,48,194,0, 0,110,255, 98,0,190, 247,0,255, 158,97,0]).view(29,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 26])\n"
     ]
    }
   ],
   "source": [
    "#pre-compute label weights\n",
    "bincount = torch.zeros(1,26).cuda()\n",
    "print(bincount.shape)\n",
    "for ii in range(len(verse19_dataset)):#without cropping\n",
    "    lab2 = torch.from_numpy(verse19_dataset[ii]['label']).long()\n",
    "    onehot = F.one_hot(lab2, 26)\n",
    "    for i in range(26):\n",
    "        if i == 0:\n",
    "            bincount[0,0] += 1/torch.bincount(onehot[:,:,:,i].reshape(-1))[1]\n",
    "            continue\n",
    "        count = torch.bincount(onehot[:,:,:,i].reshape(-1))\n",
    "        if len(count) == 2:\n",
    "            bincount[0,i] += 1/count[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-thailand",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "backbone.cuda()\n",
    "aspp.cuda()\n",
    "head.cuda()\n",
    "optimizer = torch.optim.Adam(list(backbone.parameters())+list(aspp.parameters())+list(head.parameters()),lr=0.0001)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "ts = time.time()\n",
    "run_loss = torch.zeros(4000)\n",
    "B = 4\n",
    "D,H,W = (128,128,128)\n",
    "\n",
    "for i in range(4000):\n",
    "    t0 = time.time()\n",
    "    #random mini-batch\n",
    "    idx = torch.randperm(80)[:B]\n",
    "    optimizer.zero_grad()\n",
    "    sample_img = torch.randn(B,1,D,H,W)\n",
    "    sample_lab = torch.randn(B,1,D,H,W)\n",
    "    \n",
    "    for ii in range(len(idx)):\n",
    "        sample = transformed_dataset[idx[ii]]\n",
    "        sample_img[ii] = sample[\"image\"]\n",
    "        sample_lab[ii] = sample[\"label\"]\n",
    "        \n",
    "    input = sample_img.cuda()\n",
    "    target = sample_lab.cuda().squeeze(1).long()\n",
    "    input.requires_grad = True\n",
    "    #compute the edge-term\n",
    "    edge = (torch.roll(target,1,1)!=target)\n",
    "    edge |= (torch.roll(target,1,2)!=target)\n",
    "    edge |= (torch.roll(target,1,3)!=target); \n",
    "    edge = .95+2*edge.float()\n",
    "    #forward path (including fp16 computation)\n",
    "    with torch.cuda.amp.autocast():\n",
    "        x1 = checkpoint(backbone[:2],input)\n",
    "        x2 = checkpoint(backbone[2:],x1)\n",
    "        y = checkpoint(aspp,x2)\n",
    "        #skip-connection\n",
    "        y = torch.cat((x1,F.interpolate(y,scale_factor=2)),1)\n",
    "        y1 = checkpoint(head,y)\n",
    "        output = F.interpolate(y1,scale_factor=2,mode='trilinear', align_corners=False)\n",
    "        #prediction and loss computation\n",
    "        predict = torch.log_softmax(output,1) \n",
    "        loss1 = nn.NLLLoss(weight=class_weight.cuda(),reduction='none')(predict, target)\n",
    "        loss = (loss1*edge).sum()/edge.sum()\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    # Updates the scale for next iteration.\n",
    "    scaler.update()\n",
    "    run_loss[i] = loss.item()\n",
    "    optimizer.step()\n",
    "    if(i%100==9):\n",
    "        print(i,time.time()-t0,'sec','loss',run_loss[i-8:i-1].mean())\n",
    "print('total time:', time.time() - ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "toxic-oklahoma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot loss curve\n",
    "plt.plot(torch.log(F.avg_pool1d(run_loss.view(1,1,-1),15,stride=1).squeeze()));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "noble-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preload a trained model\n",
    "torch.save({'backbone':backbone.cpu().state_dict(),'aspp':aspp.cpu().state_dict(),'head':head.cpu().state_dict(),},\\\n",
    "          'mobile_aspp2_3d_verse_edge_iso_128_patch_4k.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "loaded-glossary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp = torch.load('mobile_aspp2_3d_verse_edge_iso_128_patch_4k.pth')\n",
    "backbone.load_state_dict(cp['backbone'])\n",
    "aspp.load_state_dict(cp['aspp'])\n",
    "head.load_state_dict(cp['head'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "respiratory-representative",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = VerSe_Dataset(root_dir='/share/data_zoe1/hempe/data/VerSeV2/preprocessed19/iso_15_val', \\\n",
    "                                 transform=transforms.Compose([\n",
    "                                    ZeroPad(132),\n",
    "                                    ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ordered-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate segmentation accuracy: Dice Score\n",
    "def dice_coeff(outputs, labels, max_label):\n",
    "    dice = torch.FloatTensor(max_label-1).fill_(0)\n",
    "    for label_num in range(1, max_label):\n",
    "        iflat = (outputs==label_num).view(-1).float()\n",
    "        tflat = (labels==label_num).view(-1).float()\n",
    "        intersection = torch.mean(iflat * tflat)\n",
    "        dice[label_num-1] = (2. * intersection) / (1e-8 + torch.mean(iflat) + torch.mean(tflat))\n",
    "    return dice\n",
    "\n",
    "#evaluate localisation accuracy: Centre-Mass distance to ground-truth in voxel\n",
    "def loc_distance(label, predict):\n",
    "    ##label\n",
    "    Dl, Hl, Wl = label.squeeze(0).shape\n",
    "    onehotlabel  = F.one_hot(label.long(),26)\n",
    "\n",
    "    x = torch.linspace(0,1,Dl)\n",
    "    centre_mass_z = (x.view(-1,1,1,1)*onehotlabel[0]).sum([0,1,2])/onehotlabel[0].sum([0,1,2])*Dl\n",
    "    x = torch.linspace(0,1,Hl)\n",
    "    centre_mass_y = (x.view(1,-1,1,1)*onehotlabel[0]).sum([0,1,2])/onehotlabel[0].sum([0,1,2])*Hl\n",
    "    x = torch.linspace(0,1,Wl)\n",
    "    centre_mass_x = (x.view(1,1,-1,1)*onehotlabel[0]).sum([0,1,2])/onehotlabel[0].sum([0,1,2])*Wl\n",
    "\n",
    "    centres = np.array([np.array(centre_mass_z[1:]),np.array(centre_mass_y[1:]), np.array(centre_mass_x[1:])])\n",
    "\n",
    "    ##prediction\n",
    "    onehotpredict  = F.one_hot(predict.long(),26)\n",
    "    Dp, Hp, Wp = label.squeeze(0).shape\n",
    "\n",
    "    x = torch.linspace(0,1,Dp)\n",
    "    centre_mass_z_pred = (x.view(-1,1,1,1)*onehotpredict[0]).sum([0,1,2])/onehotpredict[0].sum([0,1,2])*Dp\n",
    "    x = torch.linspace(0,1,Hp)\n",
    "    centre_mass_y_pred = (x.view(1,-1,1,1)*onehotpredict[0]).sum([0,1,2])/onehotpredict[0].sum([0,1,2])*Hp\n",
    "    x = torch.linspace(0,1,Wp)\n",
    "    centre_mass_x_pred = (x.view(1,1,-1,1)*onehotpredict[0]).sum([0,1,2])/onehotpredict[0].sum([0,1,2])*Wp\n",
    "\n",
    "    centres_pred = np.array([np.array(centre_mass_z_pred[1:]),np.array(centre_mass_y_pred[1:]), np.array(centre_mass_x_pred[1:])])\n",
    "    \n",
    "    dist = np.sqrt(np.sum((centres-centres_pred)*(centres-centres_pred), axis=0))\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for one complete img\n",
    "head.eval()\n",
    "head.cuda()\n",
    "backbone.eval()\n",
    "backbone.cuda()\n",
    "aspp.eval()\n",
    "aspp.cuda()\n",
    "\n",
    "index = 0\n",
    "sample = validation_dataset[index]\n",
    "image = sample[\"image\"]\n",
    "label = sample[\"label\"]\n",
    "C,D,H,W = image.shape\n",
    "\n",
    "\n",
    "#apply some padding to ensure that the dimensions match for different image sizes\n",
    "new_D = D + 4 - D%4\n",
    "new_H = H + 4 - H%4\n",
    "new_W = W + 4 - W%4\n",
    "pad_img = torch.zeros((1,new_D,new_H,new_W))\n",
    "pad_lab = torch.zeros((1,new_D,new_H,new_W))\n",
    "test = pad_img[:,:D,:H,:W]\n",
    "pad_img[:,:D,:H,:W] = image\n",
    "pad_lab[:,:D,:H,:W] = label\n",
    "    \n",
    "with torch.no_grad():\n",
    "    x1 = backbone[:2](pad_img.cuda().unsqueeze(0).float())\n",
    "    y = aspp(backbone[2:](x1))\n",
    "    y = torch.cat((x1,F.interpolate(y,scale_factor=2)),1)\n",
    "    output = F.interpolate(checkpoint(head,y),scale_factor=2,mode='trilinear')\n",
    "    predict = output.argmax(1).cpu().data\n",
    "    \n",
    "d = dice_coeff(predict.cpu().contiguous(), pad_lab.cpu().contiguous(), 26)\n",
    "d_bin = dice_coeff(((torch.softmax(output,1)[:,0:1])<0.5).cpu().data,torch.clamp(pad_lab.cpu().data,0,1),2)\n",
    "d_ident = dice_coeff(pad_lab.cpu().data, pad_lab.cpu().data,26)\n",
    "dist = loc_distance(pad_lab, predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
